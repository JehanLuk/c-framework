# C-Autograd â€” Autograd e Machine Learning do Zero em C

Uma implementaÃ§Ã£o **didÃ¡tica e minimalista** de um sistema de diferenciaÃ§Ã£o automÃ¡tica (*reverse-mode autodiff*, ou **autograd**) em C, com suporte a treinamento simples (ML) e base para redes neurais, visando a criaÃ§Ã£o de um framework.

Este projeto Ã© inspirado por implementaÃ§Ãµes educacionais como o **micrograd** de Andrej Karpathy, que constrÃ³i um engine de autograd inteiro em poucas linhas de cÃ³digo, permitindo treinar modelos simples com gradiente descendente.

---

## ğŸš€ O que Ã© este projeto

Este repositÃ³rio contÃ©m:

âœ”ï¸ Um **motor de autograd** em C â€” constrÃ³i um grafo computacional  
âœ”ï¸ OperaÃ§Ãµes matemÃ¡ticas bÃ¡sicas com derivadas (`add`, `sub`, `mul`, `pow`, `log`)  
âœ”ï¸ Backpropagation via topological sort  
âœ”ï¸ Loop de treinamento com gradiente descendente  
âœ”ï¸ Um exemplo de **regressÃ£o linear treinÃ¡vel**  
âœ”ï¸ Base para estender para redes neurais

ğŸ“Œ O objetivo nÃ£o Ã© competiÃ§Ã£o de performance, e sim **entendimento profundo** da lÃ³gica interna de ML.

---

## ğŸ§  O que vocÃª pode aprender com este projeto

Ele serve como uma **sala de aula prÃ¡tica viva** para:

- DiferenciaÃ§Ã£o automÃ¡tica (*reverse-mode autodiff*)
- Grafos computacionais e backpropagation
- Gradiente descendente e otimizaÃ§Ã£o
- ImplementaÃ§Ã£o de ML do zero sem dependÃªncias externas
- Fundamentos de estruturas de dados em C (ponteiros, structs, callbacks)

Esse tipo de implementaÃ§Ã£o Ã© similar Ã  base de grandes frameworks como PyTorch â€” que tambÃ©m constroem um grafo e propagam gradientes automaticamente â€” embora em C++ e com otimizaÃ§Ãµes profundas. :contentReference[oaicite:1]{index=1}

---

## ğŸ§± Como funciona por baixo dos panos

### ğŸŸ¢ NÃ³ (`Node`)

Cada operaÃ§Ã£o ou valor Ã© armazenado como um `Node`:

- `value`: valor numÃ©rico do nÃ³
- `grad`: gradiente acumulado
- `left`, `right`: nÃ³s dependentes (grafo)
- `backward`: funÃ§Ã£o que sabe como propagar gradiente local

---

### ğŸ” ConstruÃ§Ã£o do grafo e backpropagation

1. O forward constrÃ³i um grafo de dependÃªncias automaticamente  
2. A funÃ§Ã£o `topo()` ordena os nÃ³s em uma sequÃªncia vÃ¡lida  
3. `backward(loss)` caminha a lista do final para o inÃ­cio  
4. Cada nÃ³ aplica sua derivada local multiplicada pelo gradiente acumulado

---

### ğŸ“Š Treino com gradiente descendente

O loop de treinamento faz:

forward â†’ backward â†’ gradient descent step

yaml
Copiar cÃ³digo

Com a loss definida como **MSE (Mean Squared Error)**.

---

## ğŸ“¦ Exemplo de uso

No `main()`, o cÃ³digo treina um modelo simples:

```c
Node* w = node(0.5);
Node* x = node(3.0);
Node* y = node(2.0);

for (int epoch = 0; epoch < 100; epoch++) {
    Node* pred = mul(w, x);
    Node* loss = mse(pred, y);

    backward(loss);
    step(&w, 1, 0.01);

    printf("epoch %d | loss %.4f | weight %.4f\n", epoch, loss->value, w->value);
}
```

Esse exemplo aprende o melhor valor para w que aproxima y â‰ˆ w * x.

ğŸ“š OperaÃ§Ãµes suportadas
âœ” AdiÃ§Ã£o (add)
âœ” SubtraÃ§Ã£o (sub)
âœ” MultiplicaÃ§Ã£o (mul)
âœ” PotÃªncia (pow_node)
âœ” Logaritmo (log_node)

Cada uma com seu backward apropriado.

ğŸ“Œ Como compilar
Compile com:

bash
Copiar cÃ³digo
gcc -o c_autograd main.c -lm
O -lm Ã© necessÃ¡rio para a biblioteca matemÃ¡tica (pow, log).

ğŸ§­ O que vem em seguida
Este projeto jÃ¡ implementa um autograd funcional e uma forma simples de treinar parÃ¢metros. A prÃ³xima evoluÃ§Ã£o natural inclui:

[ ] Adicionar bias e mÃºltiplos parÃ¢metros
[ ] Suportar camadas e ativaÃ§Ãµes (ReLU, Sigmoid, etc.)
[ ] Construir uma rede neural multicamada (MLP)
[ ] Criar unit tests e liberar memÃ³ria corretamente
[ ] Organizar em mÃºltiplos arquivos (.h / .c)
